# Section 1: Overview

As mentioned in the test plan document, testing was implemented in a bottom-up approach. Starting with unit tests for testing the functionality of individual functions and methods, and then moving on to the integration requirements. Once all component and integration tests were finished, I moved on to system-level tests to assess the efficacy of the entire project.

All the tests used are contained under the src/test directory. The tests are grouped by test level (unit, integration, and System). In particular, every substantial class has a unit test file associated with it. This unit test file contains multiple unit tests to thoroughly validate the functions and methods in the class. Each package then has an integration test file associated with it. This integration test file contains all the integration-level tests associated with that given package. Finally, for the system tests, we have two test files. The first file tests the system with invalid parameters to ensure robust input validation. The second file stress tests the project by testing each day on the REST API server and ensuring the system works as expected. The output files of this second test file are then manually inspected and compared to the expected output.

Overall 211 tests were written across all classes and packages created in the project. When running the tests we get an average of 93% code coverage across all classes. The main areas not being tested are discussed in detail later.

# Section 2: Functional Testing

Functional testing played a significant role in ensuring the quality and reliability of the PizzaDronz project. Following a bottom-up approach, functional tests were implemented systematically post-development, aligning with the waterfall methodology used. Each module was rigorously tested against the functional requirements outlined in the project specification and software requirements documentation. Below, I have detailed some of the functional testing conducted for key packages. Please note, that this is not a comprehensive list of all functional tests written - these can be viewed in the test folder (src/test) - instead, this is a highlight of the most important ones due to time constraints.

## 2.1: OrderInformation Package

### Order Validation Testing

The validation logic for orders was a crucial aspect of the project. Functional tests systematically evaluated the conditions for valid and invalid orders. This included attributes like:

- Validity of credit card numbers using Luhn's algorithm.
- Credit card expiry dates to ensure they were in the future.
- Ensuring all pizzas in an order came from a single restaurant.
- Verifying that orders fell within the permissible date range.

The OrderUnitTest.java file contains comprehensive test cases for these validations, with test data covering edge cases such as:

- Orders with mixed items from multiple restaurants
- Invalid dates outside of the allowed range.
- Credit cards with invalid formats or expired dates.

These test cases provided a high level of confidence in the robustness of the order validation logic.

### Credit Card Testing

Input validation for credit card numbers relied on Luhn's algorithm. Functional testing systematically validated:

- Rejection of invalid card numbers.
- Acceptance of valid numbers.

Tests were implemented in CreditCardUnitTest.java, ensuring the algorithm handled all scenarios effectively.

## 2.2: RouteCalculation Package

The core pathfinding functionality of the drone was extensively tested for correctness. Functional tests included:

- Ensuring that routes avoided no-fly zones as defined in the GeoJSON data.
- Verifying that routes always return to Appleton Tower before completing deliveries.
- Checking that routes started at the source and ended within 0.00015 degrees of the destination.

Randomised functional testing was also conducted to avoid designer bias (particularly in LngLatUnitTest.java). By generating random longitude-latitude points within the boundaries of the central area, I validated the algorithm's ability to calculate valid routes in various scenarios. Property checks were derived from project requirements.

The RouteCalculatorUnitTest.java and LngLatUnitTest.java files demonstrate these systematic tests, with additional edge cases such as:

- Routes requiring navigation around densely packed no-fly zones.
- Scenarios where no valid route existed, ensuring proper handling of failures.

## 2.3: Output Package

The output functionality was validated through functional testing to ensure compliance with GeoJSON and JSON standards. Key tests included:

- Verifying the correctness of the flight path output in GeoJSON format.
- Ensuring JSON output for order details, including valid formatting of fields like orderNo, costInPence, and outcome.

Functional tests in FileWriterUnitTest.java and FlightPathPointUnitTest.java evaluated these outputs under various conditions.

## 2.4: System-Wide Functional Testing

Once unit and integration testing were completed, functional tests were scaled to system-wide tests. These included:

- Validating the full lifecycle of the drone from fetching orders, calculating routes, and exporting data.
- Testing interactions between the PizzaDrone, OrderInformation, and RouteCalculation packages under realistic conditions.

System tests (SystemTest.java & InvalidParameterSystemTest.java) provide end-to-end validation, ensuring all requirements are met cohesively.

# Section 3: Structural Testing

Throughout the testing process, a bottom-up approach was employed, starting with component-level tests and progressing through integration to system tests. Structural testing was a critical component of this process, ensuring that all parts of the code were covered effectively.

After implementing the component-level tests, I utilised coverage analysis tools to generate detailed reports. These reports highlighted uncovered lines of code, particularly within complex control flow structures such as nested conditionals and loops. Based on this feedback, additional tests were written to target these untested branches, improving overall coverage.

Structural testing focuses on deriving test cases directly from the code structure. This included ensuring full branch and conditional coverage for the pathfinding algorithm, as well as validating edge cases in the order validation and output formatting logic. This process allowed me to systematically address gaps in testing and achieve high confidence in the correctness of the implementation.

# Section 4: Manual Inspection of Result Files

Manual inspection was performed on the output files, particularly for the system tests. This used tools such as [1] to aid in visualising the output. Manual inspection was required to ensure that the output files were both in the correct format and contained valid moves. If I had a larger timescale to develop this project, I would've attempted to automate this manual inspection process. However, due to the time constraints for this project, manual inspection was sufficient alongside the other testing techniques used.

# Section 5: Formal Proofs of Algorithms

Another testing technique employed in this project is formal proofs of the most critical components to ensure they are valid. This was performed on the drone's path-finding algorithm to ensure that the drone always found the optimal route to maximise the amount of pizzas the drone could deliver. In particular, the path-finding algorithm used is A\* which has been proven to be optimal when using an admissible heuristic [2]. Part of the testing criteria of the path-finding algorithm was therefore a formal proof that the heuristic chosen was admissible. The full formal proof for the admissibility of our heuristic can be found [here](https://github.com/IainHigh/UoE-Software-Testing-CW/blob/master/Software%20Testing%20Docs/Admissible%20Heuristic%20Formal%20Proof.pdf)

# Section 6: Scaffolding

To complete the testing for this project, scaffolding had to be developed. For example in the testing of the input validation of Orders scaffolding had to be developed that represented a test set of the orders. This was used to show that an invalid order had the correct order outcome. More details about where scaffolding was used can be found in the [Test Plan](https://github.com/IainHigh/UoE-Software-Testing-CW/blob/master/Software%20Testing%20Docs/Test%Plan.md).

# Section 7: Adequacy Criteria

Due to the vast testing techniques used to validate the efficacy of our project, we need to define multiple criteria for our tests to be deemed adequate. Starting with functionality testing, the main measurement to determine whether a test suite can be deemed adequate will be if it covers all the requirements laid out for that component. For example, in the order class there needs to be rigorous testing for different valid and invalid inputs. A test suite of orders will be considered adequate when all these requirements with different types of input data are covered. A metric that can measure this simply is the code coverage percentage - which measures the percentage of the code base which a given test suite covers. For our project, since we are aiming for thorough testing we ideally want an average code coverage of above 90% meaning our tests cover 90% of the total codebase.

For the formal proof, the mathematical proof of admissibility is enough to provide me with confidence that my heuristic is admissible and so the A\* pathfinding is optimal. However, it could be improved by using an automated reasoning language such as Isabell. Using automated reasoning to confirm the formal proof would ensure that no mistakes were made in the writing out of the proof and so would give us more confidence in the optimality of the algorithm.

For manual inspection, I deemed it to be accurate when I couldn't see any mistakes on either the JSON or GeoJSON output file for all of the system test outputs. Granted this is a flawed method as humans are far from perfect for noticing mistakes, however, without peer review or methods to crowd source this is the best method I had.

# Section 8: Results of Testing

At the start of the testing process, the main objective was to get a high degree of confidence that the system was functioning as intended. By following the planned bottom-up approach, rigorous functional testing has been completed on the system at the component level. This provides strong assurance that the components are functional and provide the expected output in the majority of cases. Integration and system-level testing were also completed to ensure that components accurately communicate with each other and the control flow of the program is faultless.

In total, 211 tests have been developed for the PizzaDronz delivery service project. Code coverage was calculated for each of these tests and the results are provided below:

| Test Level  | Average Code Coverage | Cumulative Average Code Coverage |
| ----------- | --------------------- | -------------------------------- |
| Unit        | 73%                   | 73%                              |
| Integration | 56%                   | 88%                              |
| System      | 83%                   | 93%                              |

The cumulative average code coverage is the result of running the code coverage on all test levels below the current one (e.g. Unit, Unit + Integration, Unit + Integration + System respectively.) As can be seen, the code coverage of running the full test suite is above 90% and so these tests are deemed to be adequate. On manual inspection of classes that have a low code coverage, it was found that the reason was a result of defensive programming as there are lines not being covered from conditionals that should never be entered in a normal running state for the drone. Moreover, a lot of the lines not being directly covered are contained within private methods that cannot be accessed in the tests. The integration test code coverage is fairly low as it only focuses on the OrderInformation and RouteCalculation packages and although testing the majority of these packages, since it doesn't test the Output or PizzaDronz packages the average code coverage is lower.

A screenshot showing a more detailed breakdown of the code coverage for each individual class can be found [Here](https://github.com/IainHigh/UoE-Software-Testing-CW/blob/master/Software%20Testing%20Docs/CodeCoverage.png)

# Section 9: Evaluation of the Testing Process

## 9.1: Strengths of Testing Process:

One of the core strengths of the testing process for this project is its comprehensive and multi-faceted approach, leveraging multiple testing techniques to ensure a high degree of confidence in the system's functionality and reliability. By employing the Swiss Cheese Model for error catching, the process layers various types of testing - formal proofs, functional testing, automated testing, and manual inspections - to minimise the risk of undetected errors. Each type of test complements the other, ensuring that gaps left by one approach are filled by another, much like overlapping layers of safety nets.

The use of formal proofs, particularly for the admissibility of the heuristic in the pathfinding algorithm, provides a solid theoretical foundation for the correctness of one of the most critical components of the system. This formal verification is supplemented by extensive functional tests, which validate the behaviour of individual components and their adherence to project requirements. Automated testing ensures consistency and repeatability, catching issues that might not surface during manual inspection, while manual inspection serves as a final check for errors that automated systems might overlook, especially in complex or nuanced scenarios like GeoJSON output formatting.

By following a bottom-up testing approach, the process ensures that each component is rigorously validated before integrating it into the larger system. This method reduces the likelihood of propagating errors to higher levels of testing and facilitates early detection of issues, saving time and resources in the long run. The emphasis on coverage, with a cumulative average code coverage of 93% demonstrates the robustness of the test suite. Together, these strengths ensure that the system is functionally sound.

## 9.2: Code Coverage Levels

### Target Coverage Levels

The target for unit testing alone was set at 70%, ensuring all functionalities were thoroughly tested at the component level while accepting that not all components can be tested through unit testing. For the total code coverage for unit, integration and system tests, we aimed for a target of 90%. These goals balanced exhaustive testing with practical constraints, prioritising critical areas like pathfinding and order validation while recognising limitations in areas like defensive programming.

### Actual Coverage Levels

As can be seen above in section 8, the actual code coverage achieved reflects a strong and comprehensive testing process. Unit tests achieved 73% code coverage, primarily limited by defensive programming practices and inaccessible private methods. Integration tests achieved 56%, as they primarily focused on the OrderInformation and RouteCalculation packages, excluding the Output and PizzaDronz packages. System tests were highly effective, achieving 83%, validating the overall lifecycle and integration. The cumulative coverage of 93% exceeded the adequacy threshold of 90% highlighting the robustness of the testing strategy.

## 9.4: Improvements and Next Steps

Despite the high level of coverage achieved and the rigorous testing process employed, there are areas for improvement and additional steps that could enhance the quality assurance process. Many of these improvements were omitted due to the time constraints of the project.

One key improvement would be the use of an automated reasoning language such as Isabelle for formal proofs. While the current mathematical proof of the admissibility of the A\* heuristic is robust, using a tool like Isabelle would ensure that the proof is free from human error and further validate the correctness of the pathfinding algorithm. This would provide a higher level of confidence in the optimality of the drone's route-finding capabilities, particularly for edge cases.

Another significant area for improvement is the automation of manual inspections. Currently, system test output files are manually inspected to validate their correctness. Automating this process, perhaps by writing scripts to compare expected and actual outputs, would eliminate human error and improve efficiency. This would also make it easier to scale testing as the complexity of the project grows.

Additionally, the reliance on dynamic test data from the REST API endpoint presents limitations. The current test dataset, consisting of only seven restaurants, is not reflective of real-world conditions where more restaurants may be distributed across a larger area. This limited dataset reduces our ability to stress test the system's scalability and efficiency. A future improvement would involve creating a local test server with a larger and more representative dataset, allowing for more robust testing without relying on the production REST Service.

Finally, the focus of the current test suite has been primarily focused on functional testing, leaving room for exploring more advanced testing methodologies. For instance, techniques such as chaos testing or mutation testing could be introduced to simulate unexpected scenarios, measure the robustness of the system, and estimate how many bugs we have left in the code base.

While these enhancements were not feasible within the project timeline, they represent logical next steps for further development and refinement. Implementing these improvements would increase confidence in the system's reliability, scalability, and robustness, ensuring the highest quality product.

# References

[1][https://geojson.io/#map=2/0/20]
[2] [Optimality of A\* Search Algorithm](https://towardsdatascience.com/intro-to-a-search-a3dfa444ad20#:~:text=Optimality,to%20find%20the%20optimal%20path)
