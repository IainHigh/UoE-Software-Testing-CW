# Section 1: Overview

As mentioned in the test plan document, testing was implemented in a bottom-up approach. Starting on unit tests for testing the functionality of individual functions and methods, and then moving onto the integration requirements. Once all component and integration tests were finished, I moved onto system level tests to assess the efficacy of the entire project.

All the tests used are contained under the src/test directory. The tests are grouped by test level (unit, integration, and System). In particular, every substantial class has a unit test file associated with it. This unit test file contains multiple unit tests to thoroughly validate the functions and methods in the class. Each package then has an integration test file associated with it. This integration test file contains all the integration level tests associated with that given package. Finally for the system tests we have two test files. The first file tests the system with invalid parameters to ensure robust input validation. The second file stress tests the project by testing each day on the REST API server and ensuring the system works as expected. The output files of this second test file is then manually inspected and compared to the expected output.

Overall 211 tests were written across all classes and packages created in the project. When running the tests we get an average of 93% code coverage across all classes. The main areas not being tested are discussed in detail later.

# Section 2: Functional Testing
Functional testing has been thoroughly used within the testing of this project. Following a bottom-up approach 

# Section 3: Structural Testing

# Section 4: Manual Inspection of Result Files
Manual inspection was performed on the output files, particularly for the system tests. This used tools such as [1] to aid in visualising the output. Manual inspection was required to ensure that the output files were both in the correct format and contained valid moves. If I had a larger timescale to develop this project, I would've attempted to automate this manual inspection process. However, due to the time constraints for this project, manual inspection was sufficient along side the other testing techniques used. 

# Section 5: Formal Proofs of Algorithms
Another testing technique employed in this project is formal proofs of the most critical components to ensure they are valid. This was performed on the drones path finding algorithm to ensure that the drone always found the optimal route to maximise the amount of pizzas the drone can make. In particular the path finding algorithm used is A* which has been proven to be optimal when using an admissible heuristic [2]. Part of the testing criteria of the path finding algorithm was therefore a formal proof that the heuristic choosen was admissible. The full formal proof for the admissibility of our heuristic can be found [here](https://github.com/IainHigh/UoE-Software-Testing-CW/blob/master/Software%20Testing%20Docs/Admissible%20Heuristic%20Formal%20Proof.pdf)

# Section 6: Scaffolding
To complete the testing for this project, scaffolding had to be developed. For example in the testing of the input validation of Orders scaffolding had to be developed that represented a test set of the orders. This was used to show that an invalid order had the correct order outcome. More details about where scaffolding was used can be found in the [Test Plan](https://github.com/IainHigh/UoE-Software-Testing-CW/blob/master/Software%20Testing%20Docs/Test%Plan.md).

# Section 7: Adequacy Criteria
Due to the vast testing techniques used to validate the efficacy of our project, we need to define multiple criteria for our tests to be deemed adequate. Starting with functionality testing, the main measurement to determine whether a test suite can be deemed adequate will be if it covers all the requirements laid out for that component. For example, in the order class there needs to be rigorous testing for different valid and invalid inputs. A test suite of order will be considered adequate when all these requirements with different types of input data are covered. A metric that can measure this simply is the code coverage percentage - which measures the percentage of the code base which a given test suite covers. For our project, since we are aiming for thorough testing we ideally want an average code coverage of above 90% meaning our tests cover 90% of the total codebase.

For the formal proof, the mathematical proof of admissibility is enough to provide me with confidence that my heuristic is admissible and so the A* pathfinding is optimal. However, it could be improved by using a automated reasoning language such as Isabell. Using automated reasoning to confirm the formal proof would ensure that no mistakes were made in the writing out of the proof and so would give us more confidence in the optimality of the algorithm.

For manual inspection, I deemed it to be accurate when I couldn't see any mistakes on either the JSON or GeoJSON output file for all of the system test outputs. Granted this is a flawed method as humans are far from perfect from noticing mistakes, however, without peer review or methods to crowd source this is the best method I had.

# Section 8: Results of Testing
At the start of the testing process, the main objective was to get a high degree of confidence that the system is functioning as intended. By following the planned bottom-up aproach, rigorous functional testing has been completed on the system at the component level. This provides strong assurance that the components are functional and provide the expected output in the majority of cases. Integration and system-level testing were also completed to ensure that components accurately communicate with each other and the control flow of the program is faultless.

In total, 211 tests have been developed for the PizzaDronz delivery service project. Code coverage was calculated for each of these tests and the results are provided below:

| Test Level   | Average Code Coverage | Cumulative Average Code Coverage |
|--------------|------------------------|-----------------------------------|
| Unit         | 73%                   | 73%                               |
| Integration  | 56%                   | 88%                               |
| System       | 83%                   | 93%                               |

The cumulative average code coverage is the result of running the code coverage on all test levels below the current one (e.g. Unit, Unit + Integration, Unit + Integration + System respectively.) As can be seen the code coverage of running the full test suite is above 90% and so these tests are deemed to be adequate. On manual inspection of classes that have a low code coverage it was found that the reason was a result of defensive programming as there are lines not being covered from conditionals that should never be entered in a normal running state for the drone. Moreover, a lot of the lines not being directly covered are contained within private methods that cannot be accessed in the tests. The integration test code coverage is fairly low as it only focuses on the OrderInformation and RouteCalculation packages and although testing the majority of these packages, since it doesn't test the Output or PizzaDronz packages the average code coverage is lower.

A screenshot showing a more detailed breakdown of the code coverage for each individual class can be found [Here](https://github.com/IainHigh/UoE-Software-Testing-CW/blob/master/Software%20Testing%20Docs/CodeCoverage.png)

# Section 9: Evaluation of the Testing Process

- Mention swiss hole method.

# References
[1][https://geojson.io/#map=2/0/20]
[2] [Optimality of A* Search Algorithm](https://towardsdatascience.com/intro-to-a-search-a3dfa444ad20#:~:text=Optimality,to%20find%20the%20optimal%20path)
